{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - install pytorch \n",
    "# - install pytorch geometric:  https://pytorch-geometric.readthedocs.io/en/latest/notes/installation.html\n",
    "\n",
    "# install additional libraries \n",
    "!pip install ogb\n",
    "!pip install pip install pytorch-lightning\n",
    "\n",
    "!pip install seaborn \n",
    "!pip install grakel\n",
    "!pip install karateclub\n",
    "!conda install future -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install similarity forest [Isolation forest with similarity matrix as input]\n",
    "# !git clone https://github.com/sfczekalski/similarity_forest\n",
    "# !cd similarity_forest\n",
    "# !pip install ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some useful settings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import matplotlib\n",
    "dpi = 160\n",
    "format = 'pdf'\n",
    "# configure figure\n",
    "matplotlib.rcParams.update({'font.size': 13})\n",
    "# generate some new figure. for other times this path should be ignored or set as ''\n",
    "# newpath = 'icml'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset tables\n",
    "| Dataset      | Type                 |   Label  |   Meaning   |   Confirm | \n",
    "| -----------  | -----------          | ---------| ----------- | --------- |\n",
    "| NCI1         | X & Y, Molecular     |     0    | The component (non-cell growth inhibition assay) is not effective for anit-HIV | - |\n",
    "|       -      |           -          |     1    | The component (non-cell growth inhibition assay) is effective for anit-HIV | - |\n",
    "| DD           | X & Non-X, Protein   |     0    |  691 enzymes| YES|\n",
    "|       -      |           -          |     1    |  487 non-enzymes|  YES|\n",
    "| PROTEINS     | X & Non-X, Protein   |     0    |  enzymes|  YES|\n",
    "|       -      |           -          |     1    |  non-enzymes| YES|\n",
    "| IMDB-BINARY  | X & Y, Social Net    |     0    | collabration network. movie genre: action | - |\n",
    "|       -      |           -          |     1    | collabration network. movie genre: Romance| - |\n",
    "| Mutagenicity | X & Non-X, Molecular |     0    | 2401 mutagens | YES|\n",
    "|       -      |  Semnatic Anomaly    |     1    | 1936 nonmutagens | YES|\n",
    "| PTC_MR       | X & Non-X, Molecular |     0    |  non-carcinogenicity  | Too small |\n",
    "|       -      |  Semnatic Anomaly    |     1    |  carcinogenicity  |  Too small |\n",
    "| AIDS         | X & Non-X. Molecular |     0    | 400  active, molecules with activity against HIV or not | YES|\n",
    "|       -      |   Semnatic Anomaly   |     1    | 1600  inactive, molecules with activity against HIV or not | YES|\n",
    "|ENZYMES 0-1   | X & Y, Protein       |     0/1  | specific type of enzyme| Too small ? |\n",
    "|ENZYMES 2-3   | X & Y, Protein       |     2/3  | specific type of enzyme| Too small ? |\n",
    "|COLLAB  0-1   | X & Y, Social Net    |     0/1  | collabration network. High Energy Physics or Condensed Matter|-|\n",
    "|COLLAB  1-2   | X & Y, Social Net    |     1/2  | collabration network. Condensed Matter or Astro Physics|-|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from loader import *\n",
    "import networkx as nx\n",
    "from torch_geometric.utils import to_networkx\n",
    "\n",
    "\n",
    "def data_stats(data_name):\n",
    "    # get raw dataset\n",
    "    dataset = load_data(data_name, return_raw=True)\n",
    "    # split dataset by classes\n",
    "    splits = [[] for _ in range(dataset.num_classes)] \n",
    "    for data in dataset:\n",
    "        splits[data.y.item()].append(data)\n",
    "    # compute statistics by class\n",
    "    print(f'-------------------------  {data_name} --------------------------')\n",
    "    for i, split in enumerate(splits):\n",
    "        num_graph = len(split)\n",
    "        num_features = dataset.num_node_features\n",
    "        num_nodes_all = [] \n",
    "        num_edges_all = []\n",
    "        num_components = []\n",
    "        assortativities = []\n",
    "        diameters = []\n",
    "        degrees = []\n",
    "        for graph in split:\n",
    "            num_nodes_all.append(graph.num_nodes)\n",
    "            num_edges_all.append(graph.num_edges)\n",
    "            # create networkx graph\n",
    "            g = to_networkx(graph, to_undirected=True) \n",
    "            if graph.x is not None:\n",
    "                nx.set_node_attributes(g, name='label', values={i: torch.argmax(x).item() for i, x in enumerate(graph.x)})\n",
    "            else:\n",
    "                nx.set_node_attributes(g, name='label', values={i: 0 for i in range(graph.num_nodes)})\n",
    "            # compute properties of the graph\n",
    "            num_components.append(nx.number_connected_components(g))\n",
    "            assort = nx.attribute_assortativity_coefficient(g, 'label')\n",
    "            if not np.isnan(assort):\n",
    "                assortativities.append(assort)\n",
    "\n",
    "            degree = dict(g.degree()).values()\n",
    "            degree = sum(degree) / len(degree)\n",
    "            degrees.append(degree)\n",
    "            # get largest component\n",
    "            # Gcc = sorted(nx.connected_components(g), key=len, reverse=True)\n",
    "            # gaint_g = g.subgraph(Gcc[0])\n",
    "            # diameters.append(nx.radius(gaint_g))\n",
    "\n",
    "        ave_num_nodes = sum(num_nodes_all) / len(num_nodes_all)\n",
    "        ave_num_edges = sum(num_edges_all) / len(num_edges_all)\n",
    "        ave_num_components = sum(num_components) / len(num_components)\n",
    "        ave_assortativity = sum(assortativities) / (len(assortativities) + 1e-6)\n",
    "        # ave_diameter = sum(diameters) / len(diameters)\n",
    "        ave_diameter = 0\n",
    "        ave_degree = sum(degrees) / len(degrees)\n",
    "        print(\"Class %d: num_graphs=%d, num_labels=%d, ave_num_nodes=%f, ave_num_edges=%f, ave_diameter=%f, ave_degree=%f, ave_assort=%f, ave_num_comp=%f\"\n",
    "              %(i, num_graph, num_features, ave_num_nodes, ave_num_edges, ave_diameter, ave_degree, ave_assortativity, ave_num_components))\n",
    "\n",
    "# datasets =  ['DD', 'PROTEINS', 'NCI1', 'IMDB-BINARY', 'Mutagenicity', 'AIDS', 'ENZYMES', 'COLLAB']\n",
    "datasets = ['ENZYMES', 'REDDIT-MULTI-5K']\n",
    "for data_name in datasets:\n",
    "    data_stats(data_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from torch_geometric.utils import to_networkx\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "def plot_networkx(graph, role_labels, node_size=10):\n",
    "    cmap = plt.get_cmap('tab20')\n",
    "    x_range = np.linspace(0, 1, len(np.unique(role_labels)))\n",
    "    coloring = {u: cmap(x_range[i]) for i, u in enumerate(np.unique(role_labels))}\n",
    "    node_color = [coloring[role_labels[i]] for i in range(len(role_labels))]\n",
    "    nx.draw(graph, #pos=nx.layout.fruchterman_reingold_layout(graph),\n",
    "                        node_color=node_color, cmap='hot', node_size=node_size)\n",
    "\n",
    "def visualize(data_name, k, node_size):\n",
    "    dataset = load_data(data_name, return_raw=True)\n",
    "    # split samples to each class\n",
    "    class_sep_samples = {c:[] for c in range(dataset.num_classes)}\n",
    "    for sample in dataset:\n",
    "        class_sep_samples[sample.y.item()].append(sample)\n",
    "    # sample k samples from each class and visualize them use networkx\n",
    "    plt.figure(figsize=(k*3, dataset.num_classes*3))\n",
    "    for c in range(dataset.num_classes):\n",
    "        k_samples = random.sample(class_sep_samples[c], k)\n",
    "        for i, d in enumerate(k_samples):\n",
    "            if d.x is None:\n",
    "                label = np.ones(d.num_nodes)\n",
    "            else:\n",
    "                label = d.x.argmax(dim=-1).numpy()\n",
    "            G = to_networkx(d, to_undirected=True, remove_self_loops=True)\n",
    "            plt.subplot(dataset.num_classes, k, i+1+k*c)\n",
    "            plot_networkx(G, label, node_size=node_size)\n",
    "            # if i == 0:\n",
    "            plt.axis(\"on\")\n",
    "            if i == 0:\n",
    "                plt.ylabel(f'Class={c}')\n",
    "    plt.suptitle(f'Dataset: {data_name}')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'data_visualization/{data_name}.{format}', format=format)\n",
    "    #plt.savefig(os.path.join(result_dir,'roc_vs_iter-{}-{}-{}.'.format(data_name, kernel_name, detector_name)+format), format = format) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize('REDDIT-MULTI-5K', 8, node_size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize('DD', 8, node_size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize('PROTEINS', 8, node_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize('NCI1', 8, node_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize('Mutagenicity', 8, node_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize('AIDS', 8, node_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize('ENZYMES', 8, node_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize('IMDB-BINARY', 8, node_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize('COLLAB', 8, node_size=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WL/PK Kernel Experiments \n",
    "### -Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kernel import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### -Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target: a function with kernel matrix and ys as input\n",
    "# for each node in majority class, calulate a radius to nearliest , median and farest \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.manifold import TSNE, MDS\n",
    "\n",
    "# always class 0 normal class 1 anomaly\n",
    "def sort_kms(kms, ys):\n",
    "    order = np.argsort(ys)\n",
    "    ys = ys[order]\n",
    "    kms = [km[order,:][:,order] for km in kms] \n",
    "    return kms, ys\n",
    "\n",
    "def downsampling_kms(kms, ys, down_rate=1):\n",
    "    # this need to called after sort\n",
    "    index_anomaly = np.where(ys==1)[0] # always downsample label 1 anomaly class\n",
    "    sub_index_anomaly = np.random.choice(index_anomaly, int(len(index_anomaly)*down_rate), replace=False) \n",
    "    sub_index = np.concatenate([np.where(ys==0)[0], sub_index_anomaly])\n",
    "    ys_sub = ys[sub_index]\n",
    "    kms_sub = [km[sub_index,:][:,sub_index] for km in kms]  \n",
    "    return kms_sub, ys_sub\n",
    "\n",
    "def colorbar(mappable):\n",
    "    from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "    import matplotlib.pyplot as plt\n",
    "    last_axes = plt.gca()\n",
    "    ax = mappable.axes\n",
    "    fig = ax.figure\n",
    "    divider = make_axes_locatable(ax)\n",
    "    cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "    cbar = fig.colorbar(mappable, cax=cax)\n",
    "    plt.sca(last_axes)\n",
    "    return cbar\n",
    "\n",
    "def visualize_oneclass(kms, ys, gap=2):\n",
    "    # sort ys and kms \n",
    "    kms_all, ys_all = sort_kms(kms, ys)\n",
    "    sub_index = np.where(ys_all==0)[0]\n",
    "    #print(sub_index)\n",
    "    kms0 = [km[sub_index,:][:,sub_index] for i, km in enumerate(kms_all) if i%gap==0] \n",
    "    fig, axes = plt.subplots(1, len(kms0), figsize=(len(kms0)*3,3), constrained_layout=True)\n",
    "    for ii, kernel_matrix in enumerate(kms0): \n",
    "        aa = axes[ii].matshow(kernel_matrix, cmap=plt.get_cmap('RdBu').reversed())\n",
    "        if ii == len(kms0)-1: colorbar(aa)\n",
    "        axes[ii].set_xlabel('Similarity matrix for class 0')\n",
    "        axes[ii].set_title('iter={}'.format(ii*gap+1))\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    return fig, axes\n",
    "\n",
    "def visualize_disperity(kms, ys, roc_func, k=20, gap=2, downsampling=1, down_cls=0, second_cls=None): \n",
    "    if second_cls is None:\n",
    "        second_cls = 1 - down_cls    \n",
    "    \n",
    "    # sort ys and kms\n",
    "    kms_all, ys_all = sort_kms(kms, ys)\n",
    "    # get boundaries, assume data is sorted\n",
    "    diff = np.roll(ys_all, 1, axis=0) - ys_all\n",
    "    boundaries = np.nonzero(diff)[0]\n",
    "    \n",
    "    # now create a downsampled (anomaly class downsample) version \n",
    "    kms, ys = downsampling_kms(kms_all,ys_all, downsampling)\n",
    "    kms = [kms[i] for i in range(len(kms)) if i%gap==0]\n",
    "    \n",
    "    # create label for both class\n",
    "    label0 = 'Inlier' if downsampling <1 else 'Class %d'% (second_cls)\n",
    "    label1 = 'Outlier' if downsampling <1 else 'Class %d'%down_cls   \n",
    "    color0 = 'blue' if downsampling <1 else 'tab:green'\n",
    "    color1 = 'red' if downsampling <1 else 'tab:orange'\n",
    "    colors = np.array([color0, color1])\n",
    "    \n",
    "    offset = 3 if downsampling < 1 else 0\n",
    " \n",
    "    fig, axes = plt.subplots(4-offset, len(kms), figsize=(len(kms)*3,(4-offset)*3), constrained_layout=True)\n",
    "    # create figure\n",
    "    if downsampling < 1:\n",
    "        fig.suptitle(r'\\underline{\\textbf{Class\\ ' + str(down_cls) + '\\ as\\ outlier}}  with its downsampling rate ='+str(downsampling),\n",
    "                    fontsize=15, usetex=True)\n",
    "    else:\n",
    "        fig.suptitle('Full data pairwise similarity visualization', fontsize=15)\n",
    "    \n",
    "    tsne = TSNE(n_components=2, metric=\"precomputed\")\n",
    "    mds = MDS(n_components=2, dissimilarity=\"precomputed\")\n",
    "\n",
    "    # start plot each kernel matrix\n",
    "    rocs = []\n",
    "    for ii, kernel_matrix in enumerate(kms):\n",
    "        roc = roc_func(kernel_matrix, ys)['roc_auc'] \n",
    "        rocs.append(roc)\n",
    "        percentages = {0:[], 1:[]}\n",
    "        radiuses = {0:[], 1:[]}\n",
    "        for i, y in enumerate(ys):\n",
    "            similarities_to_other_nodes = kernel_matrix[i]\n",
    "            sort_index = np.argsort(similarities_to_other_nodes)[::-1][:k] # descending order\n",
    "            neighbors = ys[sort_index]\n",
    "            percentage_of_abnormal = sum(neighbors!=y)/len(neighbors)\n",
    "            percentages[y].append(percentage_of_abnormal)\n",
    "            radius = 1 - similarities_to_other_nodes[sort_index[-1]]\n",
    "            radiuses[y].append(radius)\n",
    "            \n",
    "        if downsampling == 1:\n",
    "            aa = axes[0,ii].matshow(kernel_matrix, cmap=plt.get_cmap('RdBu').reversed())\n",
    "            colorbar(aa)\n",
    "            for boundary in boundaries:\n",
    "                axes[0,ii].axhline(y=boundary, color='green', linestyle='-')\n",
    "                axes[0,ii].axvline(x=boundary, color='green', linestyle='-')\n",
    "            axes[0,ii].set_xlabel('All-graph similarity matrix')\n",
    "            axes[0,ii].set_title('iter={}'.format(ii*gap+1))\n",
    "            \n",
    "            axes[0,ii].set_xticks([boundaries[-1]])#len(ys_all)-boundaries[-1]\n",
    "            axes[0,ii].set_xticklabels(['{}  |  {}'.format(label0, label1)])\n",
    "            axes[0,ii].set_yticks([boundaries[-1]])\n",
    "            axes[0,ii].set_yticklabels(['{}  |  {}'.format(label1, label0)],rotation='vertical', va=\"center\")\n",
    "            \n",
    "            # test\n",
    "            ys =ys.astype(int)\n",
    "            mds_embs = mds.fit_transform(1-kernel_matrix)\n",
    "            cs = colors[ys]\n",
    "            perm = np.random.permutation(len(ys))\n",
    "            scatter = axes[1,ii].scatter(mds_embs[perm,0],mds_embs[perm,1], c=cs[perm], s=1, alpha=0.3)\n",
    "            # produce a legend with the unique colors from the scatter\n",
    "            axes[1,ii].scatter(0, 0, c=color0, s=1, label=label0, alpha=0.3)\n",
    "            axes[1,ii].scatter(0, 0, c=color1, s=1, label=label1, alpha=0.3)\n",
    "            axes[1,ii].legend()\n",
    "            axes[1,ii].set_title('MDS visualization')\n",
    "            \n",
    "            sns.distplot(radiuses[0], hist=True, kde=False, color=color0, label=label0, ax=axes[2,ii])\n",
    "            sns.distplot(radiuses[1], hist=True, kde=False, color=color1, label=label1, ax=axes[2,ii])\n",
    "\n",
    "            axes[2,ii].set_xlabel('Radius of {}-NN'.format(k))\n",
    "            axes[2,ii].set_xlim(0,1)\n",
    "            axes[2,ii].set_ylabel('Number of graphs')\n",
    "            axes[2,ii].legend()  \n",
    "            \n",
    "        ax = axes[3-offset,ii] if offset < 3 else axes[ii]\n",
    "        if downsampling < 1:\n",
    "            ax.set_title('iter={}, roc='.format(ii*gap+1)+ r'\\underline{\\textbf{'+ '{:.3f}'.format(roc)+'}}', usetex=True)\n",
    "        sns.distplot(percentages[0], hist=False, color=color0, label=label0, ax=ax)\n",
    "        sns.distplot(percentages[1], hist=False, color=color1, label=label1, ax=ax)  \n",
    "        \n",
    "        ax.set_xlabel('Disagree% in {}-NN'.format(k))\n",
    "        ax.set_ylabel('Density of graphs'.format(k))\n",
    "        ax.set_xlim(-0.1,1.1)\n",
    "        ax.legend()\n",
    "#     plt.tight_layout()   \n",
    "    return fig, axes  \n",
    "\n",
    "def visualize_roc_vs_iteration(kms, ys, roc_func, downsampling=1, down_cls=0, second_cls=None, repeat=5, seed=10):\n",
    "    if second_cls is None:\n",
    "        second_cls = 1 - down_cls \n",
    "\n",
    "    def plot_iters(kms, ys, downsampling, repeat, color, down_cls=0):\n",
    "        kms_all, ys_all = sort_kms(kms, ys)\n",
    "        runs = []\n",
    "        iters = np.arange(1, len(kms_all)+1)\n",
    "        for i in range(repeat):\n",
    "            kms_sub, ys_sub = downsampling_kms(kms_all,ys_all, downsampling)  \n",
    "            # get rocs\n",
    "            rocs = []\n",
    "            for ii, kernel_matrix in enumerate(kms_sub):\n",
    "                rocs.append(roc_func(kernel_matrix, ys_sub)['roc_auc'])\n",
    "            rocs = np.array(rocs)\n",
    "            plt.plot(iters, rocs, c=color,alpha=0.2)\n",
    "            runs.append(rocs)\n",
    "        runs = np.stack(runs)\n",
    "        plt.plot(iters, runs.mean(axis=0), c=color, label='class %d as outlier'%down_cls)    \n",
    "    \n",
    "    #np.random.seed(seed)\n",
    "    fig = plt.figure()\n",
    "    plot_iters(kms, ys, downsampling, repeat, color='red', down_cls=down_cls)\n",
    "    plot_iters(kms, 1-ys, downsampling, repeat, color='blue', down_cls=second_cls)\n",
    "    plt.legend()  \n",
    "    plt.xlabel(\"Number of Propagation Iteration\")\n",
    "    plt.ylabel(\"Outlier detection ROC-AUC\")\n",
    "    plt.title(\"Outlier Downsampling Rate = {}\".format(downsampling))\n",
    "#     plt.tight_layout()\n",
    "    \n",
    "def visualize_roc_vs_sampling_rate(kms, ys, roc_func, down_cls=0, second_cls=None, iteration=5, repeat=5):\n",
    "    if second_cls is None:\n",
    "        second_cls = 1 - down_cls \n",
    "\n",
    "    def plot_rates(kms, ys, down_rates, repeat, color, down_cls):\n",
    "        kms_all, ys_all = sort_kms(kms, ys)\n",
    "        runs0 = []\n",
    "        for i in range(repeat):\n",
    "            rocs0 = []\n",
    "            for down_rate in down_rates:\n",
    "                kms_sub, ys_sub = downsampling_kms(kms_all, ys_all, down_rate)\n",
    "                rocs0.append(roc_func(kms_sub[0], ys_sub)['roc_auc'])\n",
    "            rocs0 = np.array(rocs0)\n",
    "            runs0.append(rocs0)\n",
    "            plt.plot(down_rates, rocs0, c=color, alpha=0.2)\n",
    "        plt.plot(down_rates, np.stack(runs0).mean(0),  c=color, label='class %d as outlier'%down_cls)\n",
    "        \n",
    "    assert len(kms) > iteration\n",
    "    down_rates = np.arange(0.05,0.9,0.05)\n",
    "    kms = [kms[iteration]]\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    plot_rates(kms, ys, down_rates, repeat, color='red', down_cls=down_cls)\n",
    "    plot_rates(kms, 1-ys, down_rates, repeat, color='blue', down_cls=second_cls)\n",
    "    plt.legend()     \n",
    "    plt.xlabel('Outlier downsampling rate')\n",
    "    plt.ylabel('Outlier detection ROC-AUC')\n",
    "    plt.title('Propagation iteration = {}'.format(iteration))\n",
    "#     plt.tight_layout()\n",
    "    \n",
    "from torch_geometric.data import DataLoader     \n",
    "def investigate_kernel(data_name, kernel_name, detector_name, n, down_cls=1, second_cls=None, labeled=True, bin_width=0.1, anomaly_downsampling_rate=0.1, seed=15213):\n",
    "    down_rate = 1\n",
    "    k = 20\n",
    "\n",
    "    if second_cls is None:\n",
    "        second_cls = 1 - down_cls \n",
    "\n",
    "    # Load data, seed is fixed here\n",
    "    dataset = load_data(data_name, down_class=down_cls, down_rate=down_rate, second_class=second_cls, seed=seed)[2] \n",
    "    \n",
    "    # after this, the class label has been changed: 0-second_cls  1-down_cls\n",
    "    ys = torch.cat([data.y for data in dataset])\n",
    "\n",
    "    # create loader\n",
    "    loader = DataLoader(dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "    niter = n if kernel_name =='PK' else n - 1\n",
    "\n",
    "    # build model\n",
    "    model = KernelBasedGLAD(kernel=kernel_name, detector=detector_name, WL_iter=niter, \n",
    "                            PK_bin_width=bin_width, labeled=labeled, LOF_n_neighbors=20)\n",
    "    result = model.fit(loader)\n",
    "    kernel_matrices, accumulative_kernel_matrices = model.get_kernel_matrices()\n",
    "\n",
    "    # build saving directory \n",
    "    result_dir = os.path.join(kernel_name, data_name+f'-{down_cls}-{second_cls}', detector_name)  # considering the downsample part     \n",
    "    if not os.path.exists(result_dir):\n",
    "        os.makedirs(result_dir)  \n",
    "\n",
    "    # visualize maps\n",
    "    mode = 'accumulative'\n",
    "    kms = kernel_matrices if mode == 'separate' else accumulative_kernel_matrices\n",
    "    \n",
    "    gap=2\n",
    "\n",
    "    # visualize maps, full data\n",
    "    visualize_disperity(kms, ys.numpy(), model.fit_kernel_matrix, k=k, gap=gap, downsampling=1, down_cls=down_cls) \n",
    "    plt.savefig(os.path.join(result_dir,'overlap_disperity-fulldata-{}-{}-{}-bin{}.'.format(\n",
    "                                data_name, kernel_name, detector_name, bin_width)+ format), format=format)\n",
    "    plt.close()\n",
    "\n",
    "    # visualize maps downsampling = 0.1 with down_cls=0\n",
    "    visualize_disperity(kms, ys.numpy(), model.fit_kernel_matrix, k=k, gap=gap,\n",
    "                        downsampling=anomaly_downsampling_rate, down_cls=down_cls)\n",
    "    plt.savefig(os.path.join(result_dir,'overlap_disperity-downsampled(c{}-r{})-{}-{}-{}-bin{}.'.format(\n",
    "        down_cls, anomaly_downsampling_rate, data_name, kernel_name, detector_name, bin_width)+ format), format = format)\n",
    "    plt.close()\n",
    "    \n",
    "    # visualize maps downsampling = 0.1 with down_cls=1\n",
    "    # filp data label\n",
    "    visualize_disperity(kms, 1-ys.numpy(), model.fit_kernel_matrix, k=k, gap=gap,\n",
    "                        downsampling=anomaly_downsampling_rate, down_cls=second_cls)\n",
    "    plt.savefig(os.path.join(result_dir,'overlap_disperity-downsampled(c{}-r{})-{}-{}-{}-bin{}.'.format(\n",
    "        second_cls, anomaly_downsampling_rate, data_name, kernel_name, detector_name, bin_width)+format), format = format) \n",
    "    plt.close()\n",
    "    \n",
    "    # visualize roc-vs-iteration\n",
    "    visualize_roc_vs_iteration(kms, ys, model.fit_kernel_matrix, anomaly_downsampling_rate, down_cls, repeat=10)\n",
    "    plt.savefig(os.path.join(result_dir,'roc_vs_iter-{}-{}-{}.'.format(data_name, kernel_name, detector_name)+format), format = format) \n",
    "    plt.close()\n",
    "    \n",
    "    # visualize roc-vs-sampling_rate\n",
    "    visualize_roc_vs_sampling_rate(kms, ys, model.fit_kernel_matrix, down_cls, repeat=10)   \n",
    "    plt.savefig(os.path.join(result_dir,'roc_vs_downrate-{}-{}-{}.'.format(data_name, kernel_name, detector_name)+format), format = format) \n",
    "    plt.close()\n",
    "\n",
    "# investigate_kernel('PROTEINS', 'PK', 'LOF', 11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### -Experiments: Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# datasets = ['DD', 'PROTEINS', 'NCI1', 'IMDB-BINARY', 'Mutagenicity', 'PTC_MR', 'ENZYMES', 'COLLAB']\n",
    "# data_name='PROTEINS'\n",
    "# kernel_name = 'WL'\n",
    "detector_name = 'LOF'\n",
    "n=11      \n",
    "# 'DD', 'PROTEINS', 'NCI1', \n",
    "for data in ['IMDB-BINARY', 'Mutagenicity', 'AIDS', 'ENZYMES-0-1', 'ENZYMES-2-3', 'COLLAB-0-1', 'COLLAB-1-2']:\n",
    "    d = data.split('-')\n",
    "    if len(d) == 2:\n",
    "        # 'IMDB-BINARY'\n",
    "        d[0] = d[0] + '-' + d[1] \n",
    "        down_cls, second_cls = 1, 0\n",
    "    elif len(d) > 2:\n",
    "        down_cls, second_cls = int(d[1]), int(d[2])\n",
    "    else:\n",
    "        down_cls, second_cls = 1, 0\n",
    "    for kernel_name in ['WL' , 'PK']: #'PK'\n",
    "        investigate_kernel(d[0], kernel_name, detector_name, n, down_cls=down_cls, second_cls=second_cls, seed=15213)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### -Experiments: 10 seeds, 5-layer, 0.1 down-rate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_name, down_cls, second_cls, seed = 'REDDIT-MULTI-5K', 2, 0, 15213\n",
    "kernel_name = 'WL'\n",
    "\n",
    "# need to test bin_width=0.1\n",
    "\n",
    "dataset = load_data(data_name, down_class=down_cls, second_class=second_cls, down_rate=0.1, seed=seed)[2] \n",
    "loader = DataLoader(dataset, batch_size=1, shuffle=False)\n",
    "model = KernelBasedGLAD(kernel=kernel_name, detector='LOF', WL_iter=5, PK_bin_width=0.01)\n",
    "results = model.fit(loader) \n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "def run(data_name, kernel_name, detector_name='LOF', seed=15213, down_rate=0.1, down_cls=0, second_cls=None):\n",
    "    # Reset logging: Remove all handlers associated with the root logger object.\n",
    "    for handler in logging.root.handlers[:]:\n",
    "        logging.root.removeHandler(handler)\n",
    "    logging.basicConfig(format='%(message)s', level=logging.INFO, filename=f'results/{data_name}-{kernel_name}-{detector_name}.log')\n",
    "    description = f'!Data[{data_name}] DownClass[{down_cls}] SecondClass[{second_cls}] Model[{kernel_name}-{detector_name}] Seed[{seed}]' \n",
    "    logging.info('-'*50)\n",
    "    logging.info(description)\n",
    "    dataset = load_data(data_name, down_class=down_cls, second_class=second_cls, down_rate=down_rate, seed=seed)[2] \n",
    "    loader = DataLoader(dataset, batch_size=1, shuffle=False)\n",
    "    model = KernelBasedGLAD(kernel=kernel_name, detector=detector_name, WL_iter=5, PK_bin_width=0.1)\n",
    "    results = model.fit(loader) \n",
    "    logging.info(results)\n",
    "\n",
    "\n",
    "\n",
    "detector_name = 'OCSVM'\n",
    "seeds = [15213, 10086, 11777, 333, 444, 555, 666, 777, 888, 999]\n",
    "datasets = ['DD', 'PROTEINS', 'NCI1', 'IMDB-BINARY', 'Mutagenicity', 'AIDS', 'ENZYMES-0-1', 'ENZYMES-2-3', 'COLLAB-0-1', 'COLLAB-1-2'] \n",
    "\n",
    "datasets = ['REDDIT-MULTI-5K-0-1', 'REDDIT-MULTI-5K-3-4', 'REDDIT-MULTI-5K-2-3']\n",
    "# datasets = ['REDDIT-MULTI-5K-2-3']\n",
    "\n",
    "for data in datasets:\n",
    "    d = data.split('-')\n",
    "    if len(d) == 2:\n",
    "        # 'IMDB-BINARY'\n",
    "        d[0] = d[0] + '-' + d[1] \n",
    "        down_cls, second_cls = 0, 1\n",
    "    elif len(d) > 2:\n",
    "        d[0] = '-'.join(d[:-2])\n",
    "        down_cls, second_cls = int(d[-2]), int(d[-1])\n",
    "    else:\n",
    "        down_cls, second_cls = 0, 1\n",
    "    for kernel in  ['WL']:#['PK','WL']: # ['WL', 'PK']:\n",
    "        for detector_name in ['LOF', 'OCSVM']:\n",
    "            for seed in seeds:\n",
    "                run(d[0], kernel, detector_name, seed=seed, down_cls=down_cls, second_cls=second_cls)\n",
    "            for seed in seeds:\n",
    "                run(d[0], kernel, detector_name, seed=seed, down_cls=second_cls, second_cls=down_cls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph2Vec FSGD Experiments\n",
    "### -Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from embedder import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### -Experiments: benchmark, 10 seeds, 0.1 down-rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "def run(data_name, embedder_name, detector_name='LOF', seed=15213, down_rate=0.1, down_cls=0, second_cls=None):\n",
    "    # Reset logging: Remove all handlers associated with the root logger object.\n",
    "    for handler in logging.root.handlers[:]:\n",
    "        logging.root.removeHandler(handler)\n",
    "    logging.basicConfig(format='%(message)s', level=logging.INFO, filename=f'results/{data_name}-{embedder_name}-{detector_name}.log')\n",
    "    description = f'!Data[{data_name}] DownClass[{down_cls}] SecondClass[{second_cls}] Model[{embedder_name}-{detector_name}] Seed[{seed}]' \n",
    "    logging.info('-'*50)\n",
    "    logging.info(description)\n",
    "    dataset = load_data(data_name, down_class=down_cls, second_class=second_cls, down_rate=down_rate, seed=seed)[2] \n",
    "    loader = DataLoader(dataset, batch_size=1, shuffle=False)\n",
    "    model = EmbeddingBasedGLAD(embedder=embedder_name, detector=detector_name, G2V_wl_iter=3, normalize_embedding=False)\n",
    "    results = model.fit(loader)\n",
    "    print(results)\n",
    "    logging.info(results)\n",
    "\n",
    "\n",
    "detector_name = 'OCSVM'\n",
    "seeds = [15213, 10086, 11777, 333, 444, 555, 666, 777, 888, 999]\n",
    "datasets = ['DD', 'PROTEINS', 'NCI1', 'IMDB-BINARY', 'Mutagenicity', 'AIDS', 'ENZYMES-0-1', 'ENZYMES-2-3', 'COLLAB-0-1', 'COLLAB-1-2']\n",
    "\n",
    "datasets = ['REDDIT-MULTI-5K-0-1', 'REDDIT-MULTI-5K-3-4','REDDIT-MULTI-5K-2-3']\n",
    "#datasets = ['COLLAB-1-2', 'REDDIT-MULTI-5K-2-3']\n",
    "\n",
    "\n",
    "for data in datasets:\n",
    "    d = data.split('-')\n",
    "    if len(d) == 2:\n",
    "        # 'IMDB-BINARY'\n",
    "        d[0] = d[0] + '-' + d[1] \n",
    "        down_cls, second_cls = 0, 1\n",
    "    elif len(d) > 2:\n",
    "        d[0] = '-'.join(d[:-2])\n",
    "        down_cls, second_cls = int(d[-2]), int(d[-1])\n",
    "    else:\n",
    "        down_cls, second_cls = 0, 1\n",
    "    for kernel in  ['Graph2Vec']: # ['Graph2Vec', 'FGSD']: # ['WL', 'PK']:\n",
    "        for detector_name in ['IF']: # ['LOF', 'OCSVM']:\n",
    "            for seed in seeds:\n",
    "                run(d[0], kernel, detector_name, seed=seed, down_cls=down_cls, second_cls=second_cls)\n",
    "            for seed in seeds:\n",
    "                run(d[0], kernel, detector_name, seed=seed, down_cls=second_cls, second_cls=down_cls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### -Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: T-SNE embedding visualization\n",
    "from sklearn.manifold import TSNE, MDS\n",
    "def investigate_embedder(data_name, embedder_name, detector_name='LOF', down_class=1, second_class=0, seed=15213):\n",
    "    k=20\n",
    "    if second_class is None:\n",
    "        second_class = 1 - down_class \n",
    "\n",
    "    # Load data, seed is fixed here\n",
    "    dataset = load_data(data_name, down_class=down_class, second_class=second_class, down_rate=1, seed=seed)[2] \n",
    "    \n",
    "    # after this, the class label has been changed: 0-second_cls  1-down_cls\n",
    "    ys = torch.cat([data.y for data in dataset]).numpy().astype(int)\n",
    "\n",
    "    # create loader\n",
    "    loader = DataLoader(dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "    # build model\n",
    "    model = EmbeddingBasedGLAD(embedder=embedder_name, detector=detector_name, G2V_wl_iter=3)\n",
    "    result = model.fit(loader)\n",
    "    embeds =  model.embedder.get_embedding()\n",
    "\n",
    "    # generate pairwise similarity matrix\n",
    "    # 1. sort embeds centers and ys\n",
    "    order = np.argsort(ys)\n",
    "    ys = ys[order]\n",
    "    embeds = embeds[order,:]\n",
    "\n",
    "    # 2. get boundaries, assume data is sorted\n",
    "    diff = np.roll(ys, 1, axis=0) - ys\n",
    "    boundaries = np.nonzero(diff)[0]\n",
    "    # 3. calculate pairwise similarity\n",
    "    kernel_matrix = euclidean(embeds)\n",
    "\n",
    "    # create label for both class\n",
    "    label0 = 'Class %d'% second_class\n",
    "    label1 = 'Class %d'% down_class\n",
    "    color0 = 'tab:green'\n",
    "    color1 = 'tab:orange'\n",
    "    colors = np.array([color0, color1])\n",
    "\n",
    "    percentages = {0:[], 1:[]}\n",
    "    radiuses = {0:[], 1:[]}\n",
    "    # overlap counting\n",
    "    for i, y in enumerate(ys):\n",
    "        similarities_to_other_nodes = kernel_matrix[i]\n",
    "        sort_index = np.argsort(similarities_to_other_nodes)[::-1][:k] # descending order\n",
    "        neighbors = ys[sort_index]\n",
    "        percentage_of_abnormal = sum(neighbors!=y)/len(neighbors)\n",
    "        percentages[y].append(percentage_of_abnormal)\n",
    "        radius = 1 - similarities_to_other_nodes[sort_index[-1]]\n",
    "        radiuses[y].append(radius)\n",
    "\n",
    "    plt.figure(figsize=(5*4, 5))\n",
    "    # plot matrix\n",
    "    plt.subplot(1,4,1)\n",
    "    ax = plt.gca()\n",
    "    colorbar(ax.matshow(kernel_matrix, cmap=plt.get_cmap('RdBu').reversed()))\n",
    "    for boundary in boundaries:\n",
    "        plt.axhline(y=boundary, color='green', linestyle='-')\n",
    "        plt.axvline(x=boundary, color='green', linestyle='-')\n",
    "    plt.xlabel('All-graph similarity matrix')\n",
    "    ax.set_xticks([boundaries[-1]])#len(ys_all)-boundaries[-1]\n",
    "    ax.set_xticklabels(['{}  |  {}'.format(label0, label1)])\n",
    "    ax.set_yticks([boundaries[-1]])\n",
    "    ax.set_yticklabels(['{}  |  {}'.format(label1, label0)],rotation='vertical', va=\"center\")\n",
    "\n",
    "    # MDS visualization\n",
    "    plt.subplot(1,4,2)\n",
    "    mds = MDS(n_components=2, dissimilarity=\"precomputed\")\n",
    "    mds_embs = mds.fit_transform(1-kernel_matrix)\n",
    "    cs = colors[ys]\n",
    "    perm = np.random.permutation(len(ys))\n",
    "    plt.scatter(mds_embs[perm,0],mds_embs[perm,1], c=cs[perm], s=1, alpha=0.3)\n",
    "    # produce a legend with the unique colors from the scatter\n",
    "    plt.scatter(0, 0, c=color0, s=1, label=label0, alpha=0.3)\n",
    "    plt.scatter(0, 0, c=color1, s=1, label=label1, alpha=0.3)\n",
    "    plt.legend()\n",
    "    plt.title('MDS visualization')\n",
    "\n",
    "    plt.subplot(1,4,3)\n",
    "    ax = plt.gca()\n",
    "    sns.distplot(radiuses[0], hist=True, kde=False, color=color0, label=label0, ax=ax)\n",
    "    sns.distplot(radiuses[1], hist=True, kde=False, color=color1, label=label1, ax=ax)\n",
    "    ax.set_xlabel('Radius of {}-NN'.format(k))\n",
    "    ax.set_xlim(0,1)\n",
    "    ax.set_ylabel('Number of graphs')\n",
    "    ax.legend()  \n",
    "        \n",
    "    plt.subplot(1,4,4)\n",
    "    ax = plt.gca()\n",
    "    sns.distplot(percentages[0], hist=False, color=color0, label=label0, ax=ax)\n",
    "    sns.distplot(percentages[1], hist=False, color=color1, label=label1, ax=ax)  \n",
    "    ax.set_xlabel('Disagree% in {}-NN'.format(k))\n",
    "    ax.set_ylabel('Density of graphs'.format(k))\n",
    "    ax.set_xlim(-0.1,1.1)\n",
    "    ax.legend()\n",
    "    plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seeds = [15213, 10086, 11777, 333, 444, 555, 666, 777, 888, 999]\n",
    "datasets = ['DD', 'PROTEINS', 'NCI1', 'IMDB-BINARY', 'Mutagenicity', 'AIDS', 'ENZYMES-0-1', 'ENZYMES-2-3', 'COLLAB-0-1', 'COLLAB-1-2']\n",
    "detector_name = 'LOF'\n",
    "for data in datasets:\n",
    "    d = data.split('-')\n",
    "    if len(d) == 2:\n",
    "        # 'IMDB-BINARY'\n",
    "        d[0] = d[0] + '-' + d[1] \n",
    "        down_cls, second_cls = 1, 0\n",
    "    elif len(d) > 2:\n",
    "        down_cls, second_cls = int(d[1]), int(d[2])\n",
    "    else:\n",
    "        down_cls, second_cls = 1, 0\n",
    "    for kernel in  ['Graph2Vec', 'FGSD']: \n",
    "        result_dir = os.path.join(kernel, data, detector_name)  # considering the downsample part  \n",
    "        if not os.path.exists(result_dir):\n",
    "            os.makedirs(result_dir)  \n",
    "        investigate_embedder(d[0], kernel, detector_name, down_cls, second_cls)\n",
    "        plt.savefig(os.path.join(result_dir,'overlap_disperity-fulldata-{}-{}-{}.'.format(\n",
    "                                    data, kernel, detector_name)+ format), format=format)\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.decomposition import PCA\n",
    "# from sklearn.preprocessing import RobustScaler, StandardScaler, Normalizer\n",
    "# # PCA(n_components=10).fit_transform(embeds)\n",
    "# kernel_matrix = euclidean(embeds/np.linalg.norm(embeds, axis=1, keepdims=True))\n",
    "# # kernel_matrix = euclidean(RobustScaler().fit_transform(embeds))\n",
    "# # kernel_matrix = euclidean(StandardScaler().fit_transform(embeds))\n",
    "# # kernel_matrix = euclidean(Normalizer().fit_transform(embeds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OCGIN Experiments\n",
    "### -Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ocgin import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### -Train Model and Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model and save it down\n",
    "import logging\n",
    "import os, numpy as np\n",
    "import shutil\n",
    "import pytorch_lightning as pl\n",
    "    \n",
    "def train_ocgin(data_name, down_class, nlayer, second_class=None, max_epoch=25, seed=1213, down_rate=0.1):\n",
    "    # create loader\n",
    "    loaders = create_loaders(data_name, batch_size=32, down_class=down_class, second_class=second_class, down_rate=down_rate, seed=seed)\n",
    "\n",
    "    # create model\n",
    "    model = OCGIN(loaders[3][0].num_features, weight_decay=5e-4, nlayer=nlayer)\n",
    "    # setup\n",
    "    if second_class is None:\n",
    "        second_class = 1 - down_class\n",
    "    \n",
    "    working_dir = os.path.join(type(model).__name__,\n",
    "                               f'{data_name}-{down_class}-{second_class}',\n",
    "                               'nlayer-%d'%nlayer,\n",
    "                               'seed-%d'%seed)    \n",
    "    shutil.rmtree(working_dir, ignore_errors=True)\n",
    "\n",
    "    # Reset logging: Remove all handlers associated with the root logger object.\n",
    "    for handler in logging.root.handlers[:]:\n",
    "        logging.root.removeHandler(handler)\n",
    "    # logging.getLogger('lightning').setLevel(0)\n",
    "    logging.basicConfig(format='%(message)s', level=logging.INFO, filename=f'results/{data_name}-OCGIN.log')\n",
    "    description = f'!Data[{data_name}] DownClass[{down_class}] SecondClass[{second_class}] Model[OCGIN-{nlayer}] Seed[{seed}]' \n",
    "    logging.info('-'*50)\n",
    "    logging.info(description)\n",
    "\n",
    "    # save initialized model, and save center\n",
    "    trainer0 = pl.Trainer(gpus=1, max_epochs=1, logger=False, weights_summary=None) # 1 epcoh to initialize the center of GIN\n",
    "    trainer0.fit(model, loaders[0]) \n",
    "    result0 = trainer0.test(test_dataloaders=loaders[2])[0]\n",
    "    trainer0.save_checkpoint(working_dir+'/epoch=0-val_roc_auc={:.3f}.ckpt'.format(result0['roc_auc']))\n",
    "\n",
    "    # train\n",
    "    trainer = pl.Trainer(gpus=1, max_epochs=max_epoch, default_root_dir=working_dir, weights_summary=None)\n",
    "    \n",
    "    trainer.fit(model, loaders[0]) # only use training set, no validation is used\n",
    "    result25 = trainer.test(test_dataloaders=loaders[2])[0]\n",
    "    trainer.save_checkpoint(working_dir+'/epoch=25-val_roc_auc={:.3f}.ckpt'.format(result25['roc_auc']))\n",
    "    logging.info(\"epoch0:\"+str(result0))\n",
    "    logging.info(result25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seeds = [15213, 10086, 11777, 333, 444, 555, 666, 777, 888, 999]\n",
    "datasets = ['DD', 'PROTEINS', 'NCI1', 'IMDB-BINARY', 'Mutagenicity', 'AIDS', 'ENZYMES-0-1', 'ENZYMES-2-3', 'COLLAB-0-1', 'COLLAB-1-2']\n",
    "\n",
    "datasets = ['REDDIT-BINARY', 'REDDIT-MULTI-5K-0-1', 'REDDIT-MULTI-5K-3-4', 'REDDIT-MULTI-5K-1-3']\n",
    "\n",
    "for data in datasets:\n",
    "    d = data.split('-')\n",
    "    if len(d) == 2:\n",
    "        # 'IMDB-BINARY'\n",
    "        d[0] = d[0] + '-' + d[1] \n",
    "        down_cls, second_cls = 0, 1\n",
    "    elif len(d) > 2:\n",
    "        d[0] = '-'.join(d[:-2])\n",
    "        down_cls, second_cls = int(d[-2]), int(d[-1])\n",
    "    else:\n",
    "        down_cls, second_cls = 0, 1\n",
    "\n",
    "    for seed in seeds:\n",
    "        train_ocgin(d[0], down_class=down_cls, second_class=second_cls, nlayer=5, seed=seed)\n",
    "    for seed in seeds:\n",
    "        train_ocgin(d[0], down_class=second_cls, second_class=down_cls, nlayer=5, seed=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### -Visualization Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from sklearn.manifold import TSNE, MDS\n",
    "\n",
    "def euclidean(embeddings):\n",
    "    #embeddings = embeddings - embeddings.mean(0, keepdims=True)\n",
    "    distances = euclidean_distances(embeddings, embeddings, squared=False)\n",
    "    distances /= distances.max() # between 0 and 1\n",
    "    return 1 - distances # similarity\n",
    "\n",
    "def sort_embeds(embeds, ys):\n",
    "    order = np.argsort(ys)\n",
    "    ys = ys[order]\n",
    "    embeds = embeds[:, order,:]\n",
    "    return embeds, ys\n",
    "\n",
    "def downsampling_embeds(embeds, ys, down_rate=1):\n",
    "    # this need to called after sort\n",
    "    index_anomaly = np.where(ys==1)[0]\n",
    "    sub_index_anomaly = np.random.choice(index_anomaly, int(len(index_anomaly)*down_rate), replace=False) \n",
    "    sub_index = np.concatenate([np.where(ys==0)[0], sub_index_anomaly])\n",
    "    ys_sub = ys[sub_index]\n",
    "    embeds_sub =  embeds[:, sub_index, :] \n",
    "    return embeds_sub, ys_sub\n",
    "\n",
    "def get_performance(embed, ys, center):\n",
    "    anomaly_scores = ((embed - center)**2).sum(1)\n",
    "    return roc_auc_score(ys, anomaly_scores)\n",
    "\n",
    "def colorbar(mappable):\n",
    "    from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "    import matplotlib.pyplot as plt\n",
    "    last_axes = plt.gca()\n",
    "    ax = mappable.axes\n",
    "    fig = ax.figure\n",
    "    divider = make_axes_locatable(ax)\n",
    "    cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "    cbar = fig.colorbar(mappable, cax=cax)\n",
    "    plt.sca(last_axes)\n",
    "    return cbar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "def load_model_get_embedding(data_name, down_class, second_class, nlayer, epoch, seed, Model=OCGIN, down_rate=1):\n",
    "    working_dir = os.path.join(Model.__name__, f'{data_name}-{down_class}-{second_class}',\n",
    "                               'nlayer-%d'%nlayer, 'seed-%d'%seed)                                \n",
    "    ckpt = glob.glob(os.path.join(working_dir,'*epoch=%d-*'%epoch))[0]\n",
    "    m = Model.load_from_checkpoint(ckpt)\n",
    "    m.eval()\n",
    "    m.cuda()\n",
    "    \n",
    "    # load all data without downsampling, but do set anomaly class as down_class\n",
    "    dataset = load_data(data_name, down_class=down_class, second_class=second_class, down_rate=down_rate, seed=seed)[2]\n",
    "    ys = torch.cat([data.y for data in dataset])\n",
    "    loader = DataLoader(dataset, batch_size=32, shuffle=False)\n",
    "    \n",
    "    # pass data into model to get embeddings\n",
    "    with torch.no_grad():\n",
    "        embeds, ys = [], []\n",
    "        for data in loader:\n",
    "            #batch_embeds, batch_stds = m(data.to(device))\n",
    "            embeds.append(m.get_hiddens(data.to(torch.device(\"cuda\"))))\n",
    "            ys.append(data.y)\n",
    "            \n",
    "    embeds = torch.cat(embeds, dim=1).cpu().numpy()\n",
    "    embeds = np.add.accumulate(embeds, axis=0)\n",
    "    ys = torch.cat(ys).cpu().numpy()\n",
    "    \n",
    "    # get centers for each accumulative embeddings\n",
    "    centers = m.all_layer_centers.cpu().numpy()\n",
    "    centers = np.add.accumulate(centers, axis=0)\n",
    "    return embeds, ys, centers\n",
    "\n",
    "\n",
    "def visualize_disperity_full(data_name, down_class, second_class, nlayer, epoch, seed, Model=OCGIN, gap=1):\n",
    "    embeds, ys, centers = load_model_get_embedding(data_name, down_class, second_class, nlayer, epoch, seed, Model=Model, down_rate=1)\n",
    "    roc_func=get_performance\n",
    "    k=20\n",
    "\n",
    "    # sort embeds centers and ys\n",
    "    embeds_all, ys_all = sort_embeds(embeds, ys)\n",
    "     \n",
    "    # get boundaries, assume data is sorted\n",
    "    diff = np.roll(ys_all, 1, axis=0) - ys_all\n",
    "    boundaries = np.nonzero(diff)[0]\n",
    "    \n",
    "    # now create a downsampled (anomaly class downsample) version \n",
    "    embeds_sub, ys_sub = downsampling_embeds(embeds_all, ys_all, 1)\n",
    "    \n",
    "    # only evaluate the one between gap\n",
    "    embeds_sub = np.stack([embeds_sub[i] for i in range(len(embeds_sub)) if i%gap==0])\n",
    "    centers_sub = np.stack([centers[i] for i in range(len(centers)) if i%gap==0])\n",
    "    \n",
    "    # create label for both class\n",
    "    label0 = 'Class %d'% second_class\n",
    "    label1 = 'Class %d'% down_class\n",
    "    color0 = 'tab:green'\n",
    "    color1 = 'tab:orange'\n",
    "    colors = np.array([color0, color1])\n",
    "    \n",
    "    offset = 0\n",
    "    fig, axes = plt.subplots(4-offset, len(embeds_sub), figsize=(len(embeds_sub)*3,(4-offset)*3), constrained_layout=True)\n",
    "    # create figure\n",
    "    fig.suptitle('Full data pairwise similarity visualization',fontsize=15)\n",
    "          \n",
    "    # visualize\n",
    "    for ii, embed in enumerate(embeds_sub):\n",
    "        roc = roc_func(embed, ys_sub, centers_sub[ii])\n",
    "        percentages = {0:[], 1:[]}\n",
    "        radiuses = {0:[], 1:[]}\n",
    "        # calculate similarity matrix\n",
    "        kernel_matrix = euclidean(embed)\n",
    "        kernel_matrix_all = euclidean(embeds_all[ii*gap])\n",
    "        \n",
    "        # overlap counting\n",
    "        for i, y in enumerate(ys_sub):\n",
    "            similarities_to_other_nodes = kernel_matrix[i]\n",
    "            sort_index = np.argsort(similarities_to_other_nodes)[::-1][:k] # descending order\n",
    "            neighbors = ys_sub[sort_index]\n",
    "            percentage_of_abnormal = sum(neighbors!=y)/len(neighbors)\n",
    "            percentages[y].append(percentage_of_abnormal)\n",
    "            radius = 1 - similarities_to_other_nodes[sort_index[-1]]\n",
    "            radiuses[y].append(radius)\n",
    "            \n",
    "    \n",
    "        # plot matrix\n",
    "        aa = axes[0,ii].matshow(kernel_matrix_all, cmap=plt.get_cmap('RdBu').reversed())\n",
    "        colorbar(aa)\n",
    "        for boundary in boundaries:\n",
    "            axes[0,ii].axhline(y=boundary, color='green', linestyle='-')\n",
    "            axes[0,ii].axvline(x=boundary, color='green', linestyle='-')\n",
    "        axes[0,ii].set_title('#layer={}'.format(ii*gap))\n",
    "        axes[0,ii].set_xlabel('All-graph similarity matrix')\n",
    "        \n",
    "        axes[0,ii].set_xticks([boundaries[-1]])#len(ys_all)-boundaries[-1]\n",
    "        axes[0,ii].set_xticklabels(['{}  |  {}'.format(label0, label1)])\n",
    "        axes[0,ii].set_yticks([boundaries[-1]])\n",
    "        axes[0,ii].set_yticklabels(['{}  |  {}'.format(label1, label0)],rotation='vertical', va=\"center\")\n",
    "        \n",
    "        # mds embedding           \n",
    "        ys = ys.astype(int)\n",
    "        mds = MDS(n_components=2, dissimilarity=\"precomputed\")\n",
    "        mds_embs = mds.fit_transform(1-kernel_matrix)\n",
    "\n",
    "        # mds = MDS(n_components=2)\n",
    "        # tsne = TSNE(n_components=2)\n",
    "        # mds_embs = tsne.fit_transform(embed.astype(np.float64))\n",
    "\n",
    "        cs = colors[ys]\n",
    "        perm = np.random.permutation(len(ys))\n",
    "        axes[1,ii].scatter(mds_embs[perm,0],mds_embs[perm,1], c=cs[perm], s=1, alpha=0.3)\n",
    "        # produce a legend with the unique colors from the scatter\n",
    "        axes[1,ii].scatter(0, 0, c=color0, s=1, label=label0, alpha=0.3)\n",
    "        axes[1,ii].scatter(0, 0, c=color1, s=1, label=label1, alpha=0.3)\n",
    "        axes[1,ii].legend()\n",
    "        axes[1,ii].set_title('MDS visualization')\n",
    "        \n",
    "        sns.distplot(radiuses[0], hist=True,kde=False, color=color0, label=label0, ax=axes[2,ii])\n",
    "        sns.distplot(radiuses[1], hist=True,kde=False, color=color1, label=label1, ax=axes[2,ii])\n",
    "        axes[2,ii].set_xlabel('Radius of {}-NN'.format(k))\n",
    "        axes[2,ii].set_xlim(0,1)\n",
    "        axes[2,ii].set_ylabel('Number of graphs')\n",
    "        axes[2,ii].legend()  \n",
    "            \n",
    "        ax = axes[3-offset,ii] if offset < 3 else axes[ii]\n",
    "        sns.distplot(percentages[0], hist=False, color=color0, label=label0, ax=ax)\n",
    "        sns.distplot(percentages[1], hist=False, color=color1, label=label1, ax=ax)  \n",
    "        \n",
    "        ax.set_xlabel('Disagree% in {}-NN'.format(k))\n",
    "        ax.set_ylabel('Density of graphs)'.format(k))\n",
    "        ax.set_xlim(-0.1,1.1)\n",
    "        ax.legend()\n",
    "    # plt.tight_layout()\n",
    "\n",
    "def visualize_disperity_downsampled(data_name, down_class, second_class, nlayer, epoch, seed, Model=OCGIN, gap=1):\n",
    "    embeds, ys, centers = load_model_get_embedding(data_name, down_class, second_class, nlayer, epoch, seed, Model=Model, down_rate=0.1)\n",
    "    # sort embeds centers and ys\n",
    "    embeds, ys = sort_embeds(embeds, ys)\n",
    "    k=20\n",
    "     \n",
    "    # get boundaries, assume data is sorted\n",
    "    diff = np.roll(ys, 1, axis=0) - ys\n",
    "    boundaries = np.nonzero(diff)[0]\n",
    "    \n",
    "    # create label for both class\n",
    "    label0, label1, color0, color1 = 'Inlier', 'Outlier', 'blue', 'red' \n",
    "    colors = np.array([color0, color1])\n",
    "    \n",
    "    embeds = np.stack([embeds[i] for i in range(len(embeds)) if i%gap==0])\n",
    "    centers = np.stack([centers[i] for i in range(len(centers)) if i%gap==0])\n",
    "    \n",
    "    fig, axes = plt.subplots(1, len(embeds), figsize=(len(embeds)*3,3), constrained_layout=True)\n",
    "    fig.suptitle(r'\\underline{\\textbf{Class\\ ' + str(down_class) + '\\ as\\ outlier}}  with its downsampling rate=0.1',\n",
    "                    fontsize=15, usetex=True)   \n",
    "      \n",
    "    # get performance\n",
    "    rocs = []\n",
    "    for ii, embed in enumerate(embeds):\n",
    "        roc = get_performance(embed, ys, centers[ii])\n",
    "        rocs.append(roc)\n",
    "        percentages = {0:[], 1:[]}\n",
    "        radiuses = {0:[], 1:[]}\n",
    "        # calculate similarity matrix\n",
    "        kernel_matrix = euclidean(embed)        \n",
    "         # overlap counting\n",
    "        for i, y in enumerate(ys):\n",
    "            similarities_to_other_nodes = kernel_matrix[i]\n",
    "            sort_index = np.argsort(similarities_to_other_nodes)[::-1][:k] # descending order\n",
    "            neighbors = ys[sort_index]\n",
    "            percentage_of_abnormal = sum(neighbors!=y)/len(neighbors)\n",
    "            percentages[y].append(percentage_of_abnormal)\n",
    "            radius = 1 - similarities_to_other_nodes[sort_index[-1]]\n",
    "            radiuses[y].append(radius)\n",
    "        ax =  axes[ii]    \n",
    "        ax.set_title('\\#layer={}, roc='.format(ii*gap)+ r'\\underline{\\textbf{'+ '{:.3f}'.format(roc)+'}}', usetex=True)\n",
    "        sns.distplot(percentages[0], hist=False, color=color0, label=label0, ax=ax)\n",
    "        sns.distplot(percentages[1], hist=False, color=color1, label=label1, ax=ax)  \n",
    "\n",
    "        ax.set_xlabel('Disagree% in {}-NN'.format(k))\n",
    "        ax.set_ylabel('Density of graphs'.format(k))\n",
    "        ax.set_xlim(-0.1,1.1)\n",
    "        ax.legend()\n",
    "\n",
    "\n",
    "def load_model_get_performance_perseed(data_name, down_class, second_class, nlayer, epoch, seed, Model=OCGIN, down_rate=0.1):\n",
    "    embeds, ys, centers = load_model_get_embedding(data_name, down_class, second_class, nlayer, epoch, seed, Model=Model, down_rate=down_rate)\n",
    "    # get performance\n",
    "    rocs = []\n",
    "    for ii, embed in enumerate(embeds):\n",
    "        roc = get_performance(embed, ys, centers[ii])\n",
    "        rocs.append(roc)\n",
    "    return rocs\n",
    "\n",
    "def load_model_get_performance(data_name, down_class, second_class, epoch, nlayer=5, mean=True, down_rate=0.1):\n",
    "    seeds = [15213, 10086, 11777, 333, 444, 555, 666, 777, 888, 999]\n",
    "    rocs = []\n",
    "    for seed in seeds:\n",
    "        rocs_seed = load_model_get_performance_perseed(data_name, down_class, second_class, nlayer, epoch, seed, down_rate=down_rate)   \n",
    "        rocs.append(rocs_seed)\n",
    "    rocs = np.array(rocs)\n",
    "    if mean is True:\n",
    "        rocs = rocs.mean(axis=0)\n",
    "    return rocs\n",
    "\n",
    "def visualize_roc_vs_iteration(data_name, down_class, second_class, nlayer=5, epoch=25, down_rate=0.1):\n",
    "    fig = plt.figure()\n",
    "    rocs0 = load_model_get_performance(data_name, down_class, second_class, epoch, mean=False, down_rate=down_rate)\n",
    "    rocs1 = load_model_get_performance(data_name, second_class, down_class, epoch, mean=False, down_rate=down_rate)\n",
    "    for roc in rocs0:\n",
    "        plt.plot(roc, c='red', alpha=0.2)\n",
    "    plt.plot(rocs0.mean(axis=0), c='red', label='class %d as outlier'%down_class) \n",
    "    for roc in rocs1:\n",
    "        plt.plot(roc, c='blue', alpha=0.2)\n",
    "    plt.plot(rocs1.mean(axis=0), c='blue', label='class %d as outlier'%second_class) \n",
    "\n",
    "    plt.legend()  \n",
    "    plt.xlabel(\"Number of layers\")\n",
    "    plt.ylabel(\"Outlier detection ROC-AUC\")\n",
    "    plt.title(\"Outlier Downsampling Rate = {}\".format(down_rate))\n",
    "\n",
    "def visualize_roc_vs_sampling_rate(data_name, down_class, second_class):\n",
    "    # This function needs running train_ocgin_without_save first\n",
    "    # read results from file\n",
    "    down_rates = np.linspace(0.05,0.85,17)\n",
    "    file_name = os.path.join('results', f'{data_name}-OCGIN-different_down_rate.log')\n",
    "    results = {rate:{} for rate in down_rates} \n",
    "    with open(file_name, 'r') as file:\n",
    "        lines = filter(lambda x:x[0]=='[' or x[0]=='!' or x[0]=='{', file.readlines())\n",
    "        for line in lines:\n",
    "            line = line.split()\n",
    "            if len(line) == 6:\n",
    "                line = [x.split('[') for x in line]\n",
    "                line = {x[0]: x[1].strip(']') for x in line}\n",
    "                key = line['!Data'] + '-' +  line['DownClass'] + '-' + line['SecondClass']\n",
    "                rate = float(line['DownRate'])\n",
    "                if not key in results[rate]:\n",
    "                    results[rate][key] = []\n",
    "            else:\n",
    "                results[rate][key].append(float(line[1].strip(',') )) \n",
    "    rocs = np.zeros((2, len(down_rates), 10))\n",
    "    for i, rate in enumerate(down_rates):\n",
    "        rocs[0, i] = np.array(results[rate][f'{data_name}-{down_class}-{second_class}'])\n",
    "        rocs[1, i] = np.array(results[rate][f'{data_name}-{second_class}-{down_class}'])\n",
    "    # plot \n",
    "    plt.figure()\n",
    "    rocs0 = rocs[0].T\n",
    "    rocs1 = rocs[1].T\n",
    "    for roc in rocs0:\n",
    "        plt.plot(down_rates, roc, c='red', alpha=0.2)\n",
    "    plt.plot(down_rates, rocs0.mean(axis=0), c='red', label='class %d as outlier'%down_class) \n",
    "    for roc in rocs1:\n",
    "        plt.plot(down_rates, roc, c='blue', alpha=0.2)\n",
    "    plt.plot(down_rates, rocs1.mean(axis=0), c='blue', label='class %d as outlier'%second_class) \n",
    "    plt.legend()\n",
    "    plt.xlabel('Outlier downsampling rate')\n",
    "    plt.ylabel('Outlier detection ROC-AUC')\n",
    "    plt.title('Number of layer = {}'.format(5)) \n",
    "\n",
    "def investigate_model_embeddings(data_name, down_class, second_class, nlayer=5, epoch=25, seed=15213, Model=OCGIN):\n",
    "    # define result saving path\n",
    "    result_dir = os.path.join('OCGIN_Plots', f'{data_name}-{down_class}-{second_class}', f'nlayer{nlayer}')\n",
    "    if not os.path.exists(result_dir):\n",
    "        os.makedirs(result_dir)\n",
    "    \n",
    "    gap=1\n",
    "    # visualize full data\n",
    "    \n",
    "    visualize_disperity_full(data_name, down_class, second_class, nlayer, epoch, seed, gap=gap)\n",
    "    plt.savefig(os.path.join(result_dir, 'overlap_disperity-fulldata-{}-{}-epoch{}-downcls{}.'.format(\n",
    "                    data_name, Model.__name__, epoch, 1)+ format), format=format)\n",
    "    plt.close()\n",
    "    # visualize downsampled data (transductive)\n",
    "    visualize_disperity_downsampled(data_name, down_class, second_class, nlayer, epoch, seed, gap=gap)\n",
    "    plt.savefig(os.path.join(result_dir,'overlap_disperity-downsampled(c{}-r{})-{}-{}-epoch{}.'.format(\n",
    "                        down_class, 0.1, data_name, Model.__name__, epoch)+ format),format=format)\n",
    "    plt.close()\n",
    "    \n",
    "    visualize_disperity_downsampled(data_name, second_class, down_class, nlayer, epoch, seed, gap=gap)\n",
    "    plt.savefig(os.path.join(result_dir,'overlap_disperity-downsampled(c{}-r{})-{}-{}-epoch{}.'.format(\n",
    "                        second_class, 0.1, data_name, Model.__name__, epoch)+ format), format=format)\n",
    "    plt.close()\n",
    "\n",
    "    # visualize roc-vs-iteration  down_rate=0.1\n",
    "    visualize_roc_vs_iteration(data_name, down_class, second_class, nlayer, epoch)\n",
    "    plt.savefig(os.path.join(result_dir,'roc_vs_iter-{}-{}-epoch{}.'.format(data_name, Model.__name__,epoch)+format), format=format)\n",
    "    plt.close()\n",
    "\n",
    "    # visualize roc-vs-samplingrate\n",
    "    visualize_roc_vs_sampling_rate(data_name, down_class, second_class)\n",
    "    plt.savefig(os.path.join(result_dir,'roc_vs_downrate-{}-{}-epoch{}.'.format(data_name, Model.__name__,epoch)+format), format=format)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### -Experiments: visualization, roc-vs-sampling_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  roc-vs-samplingrate\n",
    "import logging\n",
    "import os, numpy as np\n",
    "import shutil\n",
    "import pytorch_lightning as pl\n",
    "    \n",
    "def train_ocgin_without_save(data_name, down_class, nlayer, second_class=None, max_epoch=25, seed=1213, down_rate=0.1):\n",
    "    # create loader\n",
    "    loaders = create_loaders(data_name, batch_size=32, down_class=down_class, second_class=second_class, down_rate=down_rate, seed=seed)\n",
    "\n",
    "    # create model\n",
    "    model = OCGIN(loaders[3][0].num_features, weight_decay=5e-4, nlayer=nlayer)\n",
    "    # setup\n",
    "    if second_class is None:\n",
    "        second_class = 1 - down_class\n",
    "\n",
    "    # Reset logging: Remove all handlers associated with the root logger object.\n",
    "    for handler in logging.root.handlers[:]:\n",
    "        logging.root.removeHandler(handler)\n",
    "    # logging.getLogger('lightning').setLevel(0)\n",
    "    logging.basicConfig(format='%(message)s', level=logging.INFO, filename=f'results/{data_name}-OCGIN-different_down_rate.log')\n",
    "    description = f'!Data[{data_name}] DownClass[{down_class}] SecondClass[{second_class}] Model[OCGIN-{nlayer}] DownRate[{down_rate}] Seed[{seed}]' \n",
    "    logging.info('-'*50)\n",
    "    logging.info(description)\n",
    "\n",
    "    # save initialized model, and save center\n",
    "    trainer0 = pl.Trainer(gpus=1, max_epochs=1, logger=False, weights_summary=None) # 1 epcoh to initialize the center of GIN\n",
    "    trainer0.fit(model, loaders[0]) \n",
    "    result0 = trainer0.test(test_dataloaders=loaders[2])[0]\n",
    "\n",
    "    # train\n",
    "    trainer = pl.Trainer(gpus=1, max_epochs=max_epoch, logger=False,  weights_summary=None)\n",
    "    trainer.fit(model, loaders[0]) # only use training set, no validation is used\n",
    "    result25 = trainer.test(test_dataloaders=loaders[2])[0]\n",
    "    logging.info(\"epoch0:\"+str(result0))\n",
    "    logging.info(result25)\n",
    "\n",
    "seeds = [15213, 10086, 11777, 333, 444, 555, 666, 777, 888, 999]\n",
    "datasets = ['DD', 'PROTEINS', 'NCI1', 'IMDB-BINARY', 'Mutagenicity', 'AIDS', 'ENZYMES-0-1', 'ENZYMES-2-3', 'COLLAB-0-1', 'COLLAB-1-2']\n",
    "for data in datasets:\n",
    "    d = data.split('-')\n",
    "    if len(d) == 2:\n",
    "        # 'IMDB-BINARY'\n",
    "        d[0] = d[0] + '-' + d[1] \n",
    "        down_cls, second_cls = 0, 1\n",
    "    elif len(d) > 2:\n",
    "        down_cls, second_cls = int(d[1]), int(d[2])\n",
    "    else:\n",
    "        down_cls, second_cls = 0, 1\n",
    "    for down_rate in np.linspace(0.05,0.85,17):\n",
    "        for seed in seeds:\n",
    "            train_ocgin_without_save(d[0], down_class=down_cls, second_class=second_cls, nlayer=5, seed=seed, down_rate=down_rate)\n",
    "        for seed in seeds:\n",
    "            train_ocgin_without_save(d[0], down_class=second_cls, second_class=down_cls, nlayer=5, seed=seed, down_rate=down_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### -Experiments: visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "seeds = [15213, 10086, 11777, 333, 444, 555, 666, 777, 888, 999]\n",
    "datasets = ['DD', 'PROTEINS', 'NCI1', 'IMDB-BINARY', 'Mutagenicity', 'AIDS', 'ENZYMES-0-1', 'ENZYMES-2-3']#, 'COLLAB-0-1', 'COLLAB-1-2']\n",
    "for name in datasets:\n",
    "    d = name.split('-')\n",
    "    data_name, down_cls, second_cls = d[0], 1, 0\n",
    "    if len(d) == 2:\n",
    "        data_name = d[0] + '-' + d[1] \n",
    "    if len(d) > 2:\n",
    "        down_cls, second_cls = int(d[1]), int(d[2])\n",
    "    investigate_model_embeddings(data_name, down_cls, second_cls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine all results into table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "files = list(filter(lambda x:('log' in x) and ('different' not in x), os.listdir('results')))\n",
    "models = ['PK-LOF', 'PK-OCSVM', 'WL-LOF', 'WL-OCSVM', 'Graph2Vec-LOF', 'Graph2Vec-OCSVM', 'FGSD-LOF', 'FGSD-OCSVM', 'Graph2Vec-IF', 'FGSD-IF',  'OCGIN-5']\n",
    "results = {model:{} for model in models} \n",
    "\n",
    "for file_name in files:\n",
    "    file_name = os.path.join('results', file_name)\n",
    "    with open(file_name, 'r') as file:\n",
    "        lines = filter(lambda x:x[0]=='[' or x[0]=='!' or x[0]=='{', file.readlines())\n",
    "        for line in lines:\n",
    "            line = line.split()\n",
    "            if len(line) == 5:\n",
    "                line = [x.split('[') for x in line]\n",
    "                line = {x[0]: x[1].strip(']') for x in line}\n",
    "                key = line['!Data'] + '-' +  line['DownClass'] + '-' + line['SecondClass']\n",
    "                model = line['Model']\n",
    "                if not key in results[model]:\n",
    "                    results[model][key] = []\n",
    "            else:\n",
    "                results[model][key].append(float(line[1].strip(',') ))   \n",
    "\n",
    "import pandas as pd\n",
    "# create performance table\n",
    "rows = models\n",
    "cols1 = ['DD-0-1', 'PROTEINS-0-1', 'NCI1-0-1', 'IMDB-BINARY-0-1', 'Mutagenicity-0-1', 'AIDS-0-1', 'ENZYMES-0-1', 'ENZYMES-2-3', 'COLLAB-0-1', 'COLLAB-1-2']\n",
    "cols2 = ['DD-1-0', 'PROTEINS-1-0', 'NCI1-1-0', 'IMDB-BINARY-1-0', 'Mutagenicity-1-0', 'AIDS-1-0', 'ENZYMES-1-0', 'ENZYMES-3-2', 'COLLAB-1-0', 'COLLAB-2-1']\n",
    "cols = [sub[item] for item in range(len(cols1))  for sub in [cols1, cols2]]\n",
    "means = np.zeros((len(rows), len(cols)))\n",
    "stds = np.zeros((len(rows), len(cols)))\n",
    "for model, m_results in results.items():\n",
    "    for data, result in m_results.items():\n",
    "        result = np.array(result)\n",
    "        if (model in rows) and (data in cols):\n",
    "            means[rows.index(model), cols.index(data)] = round(result.mean(), 3) #result.mean()\n",
    "            stds[rows.index(model), cols.index(data)] = round(result.std(), 3)\n",
    "        else:\n",
    "            print(f'Not in set: Model={model}, Data={data}, ROC-Mean={result.mean()}, STD={result.std()}') \n",
    "\n",
    "means = pd.DataFrame(means, index=rows, columns=cols)\n",
    "stds =  pd.DataFrame(stds, index=rows, columns=cols)\n",
    "means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Take-aways and guidelines\n",
    "In this paper we have observed several intriguing issues with repurposing binary graph classification datasets for graph-level outlier detection. Different from point-cloud based outlier detection where the pointwise representation is given, graph-level outlier detection requires some graph embedding methods to embed the unstructured and complicated graph data into pointwise representation and then conduct outlier detection. Based on this mechanism, there are several angles to stand for the root of observed issues: 1) inherent dataset problem; 2) issues of graph embedding methods; 3) issues of outlier detector. We have found that each angle raises different questions and future works and all angles are valid and reasonable. Instead of standing on a specific angle, we aim at summarizing arguments of every angle to inspire researchers. \n",
    "\n",
    "## Inherent dataset problem\n",
    "\n",
    "## Is current graph embedding method \"good\" enough?\n",
    "\n",
    "## Is the assumption of outlier detector suitable?\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python379jvsc74a57bd06c6511d18c6a68b96179ab34987cf3af281251f3c34c6f7eb18d154074c7064b",
   "display_name": "Python 3.7.9 64-bit ('pytorch1.6': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}